{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uViZkhM23C_G"
      },
      "source": [
        "# P4 Regularización, Normalización y Aumentado de datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9FXVYYFu3C_M",
        "outputId": "2af82faa-cb1c-4890-a85a-025a23ae8388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "training set (60000, 28, 28)\n",
            "test set (10000, 28, 28)\n",
            "training set (48000, 784)\n",
            "val set (12000, 784)\n"
          ]
        }
      ],
      "source": [
        "## Importar y normalizar datos\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('test set', x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes=10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('val set', x_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL7OwfVg3C_O"
      },
      "source": [
        "## Modelo base\n",
        " Partiremos de una topología base e iremos añadiendo diferentes estrategias de regularización para mejorar el rendimiento del modelo.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RsQ3EKHE3C_P",
        "outputId": "c8f07676-2c88-407f-bafd-e921dd57d8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.8989\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94942, saving model to best_model.h5\n",
            "375/375 [==============================] - 13s 11ms/step - loss: 0.3458 - accuracy: 0.8991 - val_loss: 0.1714 - val_accuracy: 0.9494 - lr: 0.0250\n",
            "Epoch 2/25\n",
            "  9/375 [..............................] - ETA: 2s - loss: 0.1428 - accuracy: 0.9592"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "374/375 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.9607\n",
            "Epoch 2: val_accuracy improved from 0.94942 to 0.96200, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.1362 - accuracy: 0.9608 - val_loss: 0.1215 - val_accuracy: 0.9620 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "365/375 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9732\n",
            "Epoch 3: val_accuracy improved from 0.96200 to 0.96858, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0908 - accuracy: 0.9730 - val_loss: 0.1070 - val_accuracy: 0.9686 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9819\n",
            "Epoch 4: val_accuracy improved from 0.96858 to 0.97350, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0640 - accuracy: 0.9820 - val_loss: 0.0901 - val_accuracy: 0.9735 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9856\n",
            "Epoch 5: val_accuracy improved from 0.97350 to 0.97442, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0499 - accuracy: 0.9856 - val_loss: 0.0819 - val_accuracy: 0.9744 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9902\n",
            "Epoch 6: val_accuracy improved from 0.97442 to 0.97683, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0361 - accuracy: 0.9902 - val_loss: 0.0777 - val_accuracy: 0.9768 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9929\n",
            "Epoch 7: val_accuracy did not improve from 0.97683\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0273 - accuracy: 0.9929 - val_loss: 0.0759 - val_accuracy: 0.9763 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9954\n",
            "Epoch 8: val_accuracy improved from 0.97683 to 0.97950, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0713 - val_accuracy: 0.9795 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9964\n",
            "Epoch 9: val_accuracy improved from 0.97950 to 0.98108, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.0673 - val_accuracy: 0.9811 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9979\n",
            "Epoch 10: val_accuracy did not improve from 0.98108\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 0.0812 - val_accuracy: 0.9762 - lr: 0.0250\n",
            "Epoch 11/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9988\n",
            "Epoch 11: val_accuracy did not improve from 0.98108\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0719 - val_accuracy: 0.9798 - lr: 0.0250\n",
            "Epoch 12/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9997\n",
            "Epoch 12: val_accuracy did not improve from 0.98108\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0050 - accuracy: 0.9997 - val_loss: 0.0682 - val_accuracy: 0.9810 - lr: 0.0050\n",
            "Epoch 13/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9998\n",
            "Epoch 13: val_accuracy improved from 0.98108 to 0.98142, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0044 - accuracy: 0.9998 - val_loss: 0.0686 - val_accuracy: 0.9814 - lr: 0.0050\n",
            "Epoch 14/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9998\n",
            "Epoch 14: val_accuracy did not improve from 0.98142\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-03\n",
            "Epoch 15/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9998\n",
            "Epoch 15: val_accuracy improved from 0.98142 to 0.98167, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 0.0683 - val_accuracy: 0.9817 - lr: 1.0000e-03\n",
            "Epoch 16/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9998\n",
            "Epoch 16: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 2.0000e-04\n",
            "Epoch 17/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 17: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 2.0000e-04\n",
            "Epoch 18/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 18: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 4.0000e-05\n",
            "Epoch 19/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 19: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 4.0000e-05\n",
            "Epoch 20/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 20: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 21/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 21: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 22/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 22: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 23/25\n",
            "364/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 23: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 24/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 24: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 25/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9999\n",
            "Epoch 25: val_accuracy did not improve from 0.98167\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
            "Test loss: 0.06121474504470825\n",
            "Test accuracy: 0.9805999994277954\n"
          ]
        }
      ],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Umh-cao3C_P"
      },
      "source": [
        "## Regularización l2 (o l1)\n",
        "\n",
        "La regularización l2 consiste en añadir a la función de coste una penalización proporcional a la norma l2 de los pesos del modelo. De esta forma, se penaliza a los pesos que tengan un valor alto, forzando a que los pesos tengan valores pequeños. Esto se conoce como regularización l2. También podríamos hacer lo mismo con regularización l1 o con ambas (lo que se conoce como *Elastic net*)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M0g7BYQw3C_Q",
        "outputId": "eaaa6e37-b893-4a0a-98f9-079b5f6981d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 5.9437 - accuracy: 0.8849\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90450, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 5ms/step - loss: 5.8503 - accuracy: 0.8854 - val_loss: 0.9587 - val_accuracy: 0.9045 - lr: 0.0250\n",
            "Epoch 2/25\n",
            " 29/375 [=>............................] - ETA: 1s - loss: 0.8522 - accuracy: 0.9278"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "369/375 [============================>.] - ETA: 0s - loss: 0.5764 - accuracy: 0.9302\n",
            "Epoch 2: val_accuracy improved from 0.90450 to 0.93900, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.5746 - accuracy: 0.9304 - val_loss: 0.4452 - val_accuracy: 0.9390 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "363/375 [============================>.] - ETA: 0s - loss: 0.4290 - accuracy: 0.9393\n",
            "Epoch 3: val_accuracy did not improve from 0.93900\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.4295 - accuracy: 0.9391 - val_loss: 0.4306 - val_accuracy: 0.9362 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "362/375 [===========================>..] - ETA: 0s - loss: 0.4051 - accuracy: 0.9430\n",
            "Epoch 4: val_accuracy did not improve from 0.93900\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.4044 - accuracy: 0.9432 - val_loss: 0.4091 - val_accuracy: 0.9379 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.9464\n",
            "Epoch 5: val_accuracy improved from 0.93900 to 0.94775, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3871 - accuracy: 0.9462 - val_loss: 0.3797 - val_accuracy: 0.9477 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.9495\n",
            "Epoch 6: val_accuracy improved from 0.94775 to 0.94883, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.3707 - accuracy: 0.9495 - val_loss: 0.3675 - val_accuracy: 0.9488 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.9503\n",
            "Epoch 7: val_accuracy did not improve from 0.94883\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.3648 - accuracy: 0.9500 - val_loss: 0.3716 - val_accuracy: 0.9467 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.9498\n",
            "Epoch 8: val_accuracy improved from 0.94883 to 0.95425, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3580 - accuracy: 0.9501 - val_loss: 0.3444 - val_accuracy: 0.9542 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "365/375 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.9521\n",
            "Epoch 9: val_accuracy did not improve from 0.95425\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3469 - accuracy: 0.9518 - val_loss: 0.3763 - val_accuracy: 0.9428 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.9521\n",
            "Epoch 10: val_accuracy did not improve from 0.95425\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3456 - accuracy: 0.9520 - val_loss: 0.3659 - val_accuracy: 0.9481 - lr: 0.0250\n",
            "Epoch 11/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.9645\n",
            "Epoch 11: val_accuracy improved from 0.95425 to 0.96450, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3016 - accuracy: 0.9645 - val_loss: 0.3005 - val_accuracy: 0.9645 - lr: 0.0050\n",
            "Epoch 12/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9676\n",
            "Epoch 12: val_accuracy did not improve from 0.96450\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2899 - accuracy: 0.9676 - val_loss: 0.3001 - val_accuracy: 0.9611 - lr: 0.0050\n",
            "Epoch 13/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9673\n",
            "Epoch 13: val_accuracy did not improve from 0.96450\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2864 - accuracy: 0.9672 - val_loss: 0.2961 - val_accuracy: 0.9637 - lr: 0.0050\n",
            "Epoch 14/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9673\n",
            "Epoch 14: val_accuracy improved from 0.96450 to 0.96508, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2860 - accuracy: 0.9674 - val_loss: 0.2913 - val_accuracy: 0.9651 - lr: 0.0050\n",
            "Epoch 15/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9674\n",
            "Epoch 15: val_accuracy improved from 0.96508 to 0.96567, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2831 - accuracy: 0.9675 - val_loss: 0.2886 - val_accuracy: 0.9657 - lr: 0.0050\n",
            "Epoch 16/25\n",
            "364/375 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9675\n",
            "Epoch 16: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2823 - accuracy: 0.9676 - val_loss: 0.2922 - val_accuracy: 0.9643 - lr: 0.0050\n",
            "Epoch 17/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.9684\n",
            "Epoch 17: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2804 - accuracy: 0.9685 - val_loss: 0.2861 - val_accuracy: 0.9653 - lr: 0.0050\n",
            "Epoch 18/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.9681\n",
            "Epoch 18: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2798 - accuracy: 0.9679 - val_loss: 0.2881 - val_accuracy: 0.9657 - lr: 0.0050\n",
            "Epoch 19/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.9687\n",
            "Epoch 19: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2779 - accuracy: 0.9687 - val_loss: 0.2852 - val_accuracy: 0.9645 - lr: 0.0050\n",
            "Epoch 20/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.9685\n",
            "Epoch 20: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2776 - accuracy: 0.9685 - val_loss: 0.2913 - val_accuracy: 0.9644 - lr: 0.0050\n",
            "Epoch 21/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9683\n",
            "Epoch 21: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2763 - accuracy: 0.9683 - val_loss: 0.2850 - val_accuracy: 0.9656 - lr: 0.0050\n",
            "Epoch 22/25\n",
            "363/375 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9684\n",
            "Epoch 22: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2749 - accuracy: 0.9685 - val_loss: 0.2877 - val_accuracy: 0.9637 - lr: 0.0050\n",
            "Epoch 23/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9681\n",
            "Epoch 23: val_accuracy did not improve from 0.96567\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2746 - accuracy: 0.9681 - val_loss: 0.2822 - val_accuracy: 0.9651 - lr: 0.0050\n",
            "Epoch 24/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9680\n",
            "Epoch 24: val_accuracy improved from 0.96567 to 0.96708, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2748 - accuracy: 0.9681 - val_loss: 0.2800 - val_accuracy: 0.9671 - lr: 0.0050\n",
            "Epoch 25/25\n",
            "362/375 [===========================>..] - ETA: 0s - loss: 0.2716 - accuracy: 0.9688\n",
            "Epoch 25: val_accuracy did not improve from 0.96708\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2718 - accuracy: 0.9686 - val_loss: 0.2838 - val_accuracy: 0.9657 - lr: 0.0050\n",
            "Test loss: 0.27328160405158997\n",
            "Test accuracy: 0.9674000144004822\n"
          ]
        }
      ],
      "source": [
        "## Teniendo en cuenta el modelo base añade regularización L2 a las capas densas\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2,l1\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01))) #el kernel regulariza sobre los pesos no sobre el bias, el 0,01 es el lambda -> Loss + lambda ||W||2 2\n",
        "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01))) #cambiando l2 por l1 cambias de regularizacion\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rsxLNOj3C_Q"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "El dropout es una técnica de regularización que consiste en eliminar aleatoriamente un porcentaje de las neuronas de la red durante el entrenamiento. De esta forma, se evita que la red se sobreajuste a los datos de entrenamiento y se mejora la generalización del modelo.\n",
        "\n",
        "\n",
        "El dropout se carga el valor de la neurona con cierta probabilidad, entonces el valor que tenia pasa a ser cero si el dropout es de p=0.5, con probabilidad 0.5 cada neurona se pondra a 0 o mantendra su valor, se hace de manera aleatoria\n",
        "\n",
        "Con esto evitas que alguna neurona se superespecialice, evita el overfitting y se acerca a la generalizacion.\n",
        "\n",
        "en fase de entrenamiento se hace como esta aplciado arriba\n",
        "en fase inferencia todas las neuronas estan activas y para que los vlaores no sean excesivamente grandes, se multiplica cada de uno de los pesos por 1-p para reajustarlo.\n",
        "\n",
        "se hace con un if, si esta en entrenamiento aplicas dropout, sino no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GfWpdds-3C_R",
        "outputId": "76a3150a-fcd6-4550-c795-97b0fc808b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.8602\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94650, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 7ms/step - loss: 0.4537 - accuracy: 0.8606 - val_loss: 0.1844 - val_accuracy: 0.9465 - lr: 0.0250\n",
            "Epoch 2/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9404\n",
            "Epoch 2: val_accuracy improved from 0.94650 to 0.96233, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1967 - accuracy: 0.9405 - val_loss: 0.1263 - val_accuracy: 0.9623 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9560\n",
            "Epoch 3: val_accuracy improved from 0.96233 to 0.96867, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1461 - accuracy: 0.9560 - val_loss: 0.1059 - val_accuracy: 0.9687 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9643\n",
            "Epoch 4: val_accuracy improved from 0.96867 to 0.97133, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1192 - accuracy: 0.9645 - val_loss: 0.0961 - val_accuracy: 0.9713 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9686\n",
            "Epoch 5: val_accuracy improved from 0.97133 to 0.97433, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1017 - accuracy: 0.9688 - val_loss: 0.0878 - val_accuracy: 0.9743 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9726\n",
            "Epoch 6: val_accuracy improved from 0.97433 to 0.97667, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0890 - accuracy: 0.9726 - val_loss: 0.0790 - val_accuracy: 0.9767 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9745\n",
            "Epoch 7: val_accuracy improved from 0.97667 to 0.97733, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0796 - accuracy: 0.9746 - val_loss: 0.0735 - val_accuracy: 0.9773 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9778\n",
            "Epoch 8: val_accuracy improved from 0.97733 to 0.97825, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0701 - accuracy: 0.9778 - val_loss: 0.0734 - val_accuracy: 0.9783 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9790\n",
            "Epoch 9: val_accuracy improved from 0.97825 to 0.97992, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0663 - accuracy: 0.9790 - val_loss: 0.0693 - val_accuracy: 0.9799 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9813\n",
            "Epoch 10: val_accuracy did not improve from 0.97992\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0575 - accuracy: 0.9813 - val_loss: 0.0697 - val_accuracy: 0.9792 - lr: 0.0250\n",
            "Epoch 11/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9824\n",
            "Epoch 11: val_accuracy improved from 0.97992 to 0.98025, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0538 - accuracy: 0.9825 - val_loss: 0.0679 - val_accuracy: 0.9803 - lr: 0.0250\n",
            "Epoch 12/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0486 - accuracy: 0.9848\n",
            "Epoch 12: val_accuracy improved from 0.98025 to 0.98067, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0486 - accuracy: 0.9848 - val_loss: 0.0694 - val_accuracy: 0.9807 - lr: 0.0250\n",
            "Epoch 13/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0463 - accuracy: 0.9849\n",
            "Epoch 13: val_accuracy improved from 0.98067 to 0.98083, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0462 - accuracy: 0.9850 - val_loss: 0.0668 - val_accuracy: 0.9808 - lr: 0.0250\n",
            "Epoch 14/25\n",
            "364/375 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9869\n",
            "Epoch 14: val_accuracy did not improve from 0.98083\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0408 - accuracy: 0.9869 - val_loss: 0.0669 - val_accuracy: 0.9803 - lr: 0.0250\n",
            "Epoch 15/25\n",
            "365/375 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9865\n",
            "Epoch 15: val_accuracy improved from 0.98083 to 0.98158, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 0.0675 - val_accuracy: 0.9816 - lr: 0.0250\n",
            "Epoch 16/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9894\n",
            "Epoch 16: val_accuracy improved from 0.98158 to 0.98283, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0322 - accuracy: 0.9895 - val_loss: 0.0602 - val_accuracy: 0.9828 - lr: 0.0050\n",
            "Epoch 17/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.0255 - accuracy: 0.9922\n",
            "Epoch 17: val_accuracy improved from 0.98283 to 0.98317, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.0597 - val_accuracy: 0.9832 - lr: 0.0050\n",
            "Epoch 18/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9926\n",
            "Epoch 18: val_accuracy improved from 0.98317 to 0.98333, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0236 - accuracy: 0.9926 - val_loss: 0.0620 - val_accuracy: 0.9833 - lr: 0.0050\n",
            "Epoch 19/25\n",
            "362/375 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9939\n",
            "Epoch 19: val_accuracy improved from 0.98333 to 0.98350, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0614 - val_accuracy: 0.9835 - lr: 0.0050\n",
            "Epoch 20/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9934\n",
            "Epoch 20: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.0611 - val_accuracy: 0.9833 - lr: 1.0000e-03\n",
            "Epoch 21/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9934\n",
            "Epoch 21: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.0608 - val_accuracy: 0.9835 - lr: 1.0000e-03\n",
            "Epoch 22/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933\n",
            "Epoch 22: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.0608 - val_accuracy: 0.9833 - lr: 2.0000e-04\n",
            "Epoch 23/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9939\n",
            "Epoch 23: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.0608 - val_accuracy: 0.9833 - lr: 2.0000e-04\n",
            "Epoch 24/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 24: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.0608 - val_accuracy: 0.9833 - lr: 4.0000e-05\n",
            "Epoch 25/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9938\n",
            "Epoch 25: val_accuracy did not improve from 0.98350\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0608 - val_accuracy: 0.9833 - lr: 4.0000e-05\n",
            "Test loss: 0.056057482957839966\n",
            "Test accuracy: 0.9836999773979187\n"
          ]
        }
      ],
      "source": [
        "## Teniendo en cuenta el modelo base añade regularización de tipo dropout a las capas densas\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljZIPE2i3C_S"
      },
      "source": [
        "## Normalización BatchNorm\n",
        "\n",
        "La normalización BatchNorm consiste en normalizar la salida de una capa de la red neuronal para que tenga media 0 y varianza 1. De esta forma, se consigue que la red neuronal pueda entrenarse más rápido y que sea más robusta a cambios en los pesos de las capas anteriores.\n",
        "\n",
        "\n",
        "Al normalizar todos los datos q entran en la siguiente capa entran con media 0 y desviacion tipica 1. la normalizacion se hace para cada valor, se resta la media del batch y se divide por la desviacion tipica de es batch. Al dividir le sumas epsilon en el dividendo para q no divida por 0.\n",
        "\n",
        "\n",
        "!!!!Es muy buena practica noramlizar los datos despues de cada capa, asi se puede manejar de manera eficiente los datos.  Con batch pequeño puede dar problemas y hoy en dia se usa mas el layernorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "onYUgvSn3C_S",
        "outputId": "996cab98-0c80-49e9-d2f9-71180e77efcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9413\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96225, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 6ms/step - loss: 0.2078 - accuracy: 0.9414 - val_loss: 0.1284 - val_accuracy: 0.9622 - lr: 0.0250\n",
            "Epoch 2/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9796\n",
            "Epoch 2: val_accuracy improved from 0.96225 to 0.97367, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0642 - accuracy: 0.9797 - val_loss: 0.0881 - val_accuracy: 0.9737 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9883\n",
            "Epoch 3: val_accuracy improved from 0.97367 to 0.97375, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0360 - accuracy: 0.9884 - val_loss: 0.0970 - val_accuracy: 0.9737 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "363/375 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9938\n",
            "Epoch 4: val_accuracy improved from 0.97375 to 0.97850, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.0801 - val_accuracy: 0.9785 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9962\n",
            "Epoch 5: val_accuracy improved from 0.97850 to 0.98208, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.0709 - val_accuracy: 0.9821 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n",
            "Epoch 6: val_accuracy did not improve from 0.98208\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0794 - val_accuracy: 0.9815 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
            "Epoch 7: val_accuracy improved from 0.98208 to 0.98242, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0693 - val_accuracy: 0.9824 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
            "Epoch 8: val_accuracy improved from 0.98242 to 0.98375, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0730 - val_accuracy: 0.9837 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
            "Epoch 9: val_accuracy improved from 0.98375 to 0.98425, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0699 - val_accuracy: 0.9843 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 7.6947e-04 - accuracy: 0.9999\n",
            "Epoch 10: val_accuracy improved from 0.98425 to 0.98450, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 7.6674e-04 - accuracy: 0.9999 - val_loss: 0.0687 - val_accuracy: 0.9845 - lr: 0.0050\n",
            "Epoch 11/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 6.1962e-04 - accuracy: 1.0000\n",
            "Epoch 11: val_accuracy did not improve from 0.98450\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 6.2026e-04 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9843 - lr: 0.0050\n",
            "Epoch 12/25\n",
            "364/375 [============================>.] - ETA: 0s - loss: 5.2657e-04 - accuracy: 1.0000\n",
            "Epoch 12: val_accuracy did not improve from 0.98450\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 5.2447e-04 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9843 - lr: 0.0050\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 5.9527e-04 - accuracy: 1.0000\n",
            "Epoch 13: val_accuracy improved from 0.98450 to 0.98467, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 5.9527e-04 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9847 - lr: 0.0050\n",
            "Epoch 14/25\n",
            "366/375 [============================>.] - ETA: 0s - loss: 5.1180e-04 - accuracy: 1.0000\n",
            "Epoch 14: val_accuracy improved from 0.98467 to 0.98533, saving model to best_model.h5\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 5.0636e-04 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9853 - lr: 0.0050\n",
            "Epoch 15/25\n",
            "371/375 [============================>.] - ETA: 0s - loss: 4.6678e-04 - accuracy: 1.0000\n",
            "Epoch 15: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 4.6974e-04 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9847 - lr: 0.0050\n",
            "Epoch 16/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 4.4588e-04 - accuracy: 1.0000\n",
            "Epoch 16: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.5190e-04 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9849 - lr: 0.0050\n",
            "Epoch 17/25\n",
            "370/375 [============================>.] - ETA: 0s - loss: 4.3110e-04 - accuracy: 1.0000\n",
            "Epoch 17: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.3211e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9848 - lr: 1.0000e-03\n",
            "Epoch 18/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 4.8324e-04 - accuracy: 1.0000\n",
            "Epoch 18: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 4.8038e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9848 - lr: 1.0000e-03\n",
            "Epoch 19/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 4.6225e-04 - accuracy: 1.0000\n",
            "Epoch 19: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 4.6036e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9850 - lr: 2.0000e-04\n",
            "Epoch 20/25\n",
            "368/375 [============================>.] - ETA: 0s - loss: 4.3576e-04 - accuracy: 1.0000\n",
            "Epoch 20: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 4.3496e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9847 - lr: 2.0000e-04\n",
            "Epoch 21/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 4.2282e-04 - accuracy: 1.0000\n",
            "Epoch 21: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 4.2282e-04 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9847 - lr: 4.0000e-05\n",
            "Epoch 22/25\n",
            "369/375 [============================>.] - ETA: 0s - loss: 4.4197e-04 - accuracy: 1.0000\n",
            "Epoch 22: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 4.4574e-04 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9846 - lr: 4.0000e-05\n",
            "Epoch 23/25\n",
            "372/375 [============================>.] - ETA: 0s - loss: 4.4286e-04 - accuracy: 1.0000\n",
            "Epoch 23: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 4.4429e-04 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9849 - lr: 1.0000e-05\n",
            "Epoch 24/25\n",
            "367/375 [============================>.] - ETA: 0s - loss: 4.8468e-04 - accuracy: 1.0000\n",
            "Epoch 24: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 4.8060e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9847 - lr: 1.0000e-05\n",
            "Epoch 25/25\n",
            "373/375 [============================>.] - ETA: 0s - loss: 4.1264e-04 - accuracy: 1.0000\n",
            "Epoch 25: val_accuracy did not improve from 0.98533\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 4.1347e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9847 - lr: 1.0000e-05\n",
            "Test loss: 0.056488122791051865\n",
            "Test accuracy: 0.9851999878883362\n"
          ]
        }
      ],
      "source": [
        "## Teniendo en cuenta el modelo base añade normalización BatchNorm\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())#aqui igual, los argumentos pueden ser gamma y beta, que transforma gamma*X + beta\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())#con esto ya haces la normalizacion\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYc4cTft3C_S"
      },
      "source": [
        "## Aumentado de datos\n",
        "\n",
        "El aumentado de datos consiste en generar nuevos datos de entrenamiento a partir de los datos de entrenamiento originales. De esta forma, se consigue que el modelo sea más robusto y que se generalice mejor a datos que no ha visto durante el entrenamiento.\n",
        "\n",
        "En nuestro caso para los dígitos de la MNIST vamos a realizar un aumento de datos de la siguiente forma:\n",
        "\n",
        "- Rotación aleatoria de la imagen entre -30 y 30 grados.\n",
        "- Traslación aleatoria de la imagen entre -3 y 3 píxeles en horizontal y vertical.\n",
        "- Escalado aleatorio de la imagen entre 0.8 y 1.2.\n",
        "- Inversión aleatoria de la imagen en horizontal y vertical. **NO!!!** depende de la tarea en imagenes de animales puede servir.\n",
        "\n",
        "El aumentado de datos se ejecuta en CPU y ralentiza el entrenamiento.\n",
        "\n",
        "Normalmente además, se necesitarán más epochs para entrenar el modelo.\n",
        "\n",
        "\n",
        "\n",
        "Para añadir ruido a la imagen se puede hacer añadiendo gaussian noise en la capa de entrada de datos\n",
        "Cut out, sirve para tapar una parte de la imagen. asi forzar a la red a aprender.\n",
        "Puedes hacer combinaciones lineales con dos imagenes, mixup con lo que la probabilidad q sea coche y vaca sera porporcional al mix que hayas hecho\n",
        "\n",
        "Tambien se puede cargar palabras, o transponer algunas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kP9Y0_6F3C_T",
        "outputId": "cdc33898-4e57-4530-afea-fed18c86416f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.8189\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94150, saving model to best_model.h5\n",
            "375/375 [==============================] - 24s 60ms/step - loss: 0.6048 - accuracy: 0.8189 - val_loss: 0.1890 - val_accuracy: 0.9415 - lr: 0.0250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8982\n",
            "Epoch 2: val_accuracy improved from 0.94150 to 0.94942, saving model to best_model.h5\n",
            "375/375 [==============================] - 17s 46ms/step - loss: 0.3400 - accuracy: 0.8983 - val_loss: 0.1712 - val_accuracy: 0.9494 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.9154\n",
            "Epoch 3: val_accuracy improved from 0.94942 to 0.95000, saving model to best_model.h5\n",
            "375/375 [==============================] - 18s 47ms/step - loss: 0.2789 - accuracy: 0.9155 - val_loss: 0.1573 - val_accuracy: 0.9500 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9289\n",
            "Epoch 4: val_accuracy improved from 0.95000 to 0.95833, saving model to best_model.h5\n",
            "375/375 [==============================] - 17s 46ms/step - loss: 0.2420 - accuracy: 0.9289 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9322\n",
            "Epoch 5: val_accuracy improved from 0.95833 to 0.96833, saving model to best_model.h5\n",
            "375/375 [==============================] - 17s 44ms/step - loss: 0.2245 - accuracy: 0.9322 - val_loss: 0.0994 - val_accuracy: 0.9683 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9392\n",
            "Epoch 6: val_accuracy did not improve from 0.96833\n",
            "375/375 [==============================] - 16s 44ms/step - loss: 0.2033 - accuracy: 0.9392 - val_loss: 0.1175 - val_accuracy: 0.9657 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9440\n",
            "Epoch 7: val_accuracy improved from 0.96833 to 0.97192, saving model to best_model.h5\n",
            "375/375 [==============================] - 19s 51ms/step - loss: 0.1833 - accuracy: 0.9440 - val_loss: 0.0896 - val_accuracy: 0.9719 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9482\n",
            "Epoch 8: val_accuracy did not improve from 0.97192\n",
            "375/375 [==============================] - 16s 44ms/step - loss: 0.1729 - accuracy: 0.9482 - val_loss: 0.0932 - val_accuracy: 0.9703 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9489\n",
            "Epoch 9: val_accuracy improved from 0.97192 to 0.97450, saving model to best_model.h5\n",
            "375/375 [==============================] - 17s 44ms/step - loss: 0.1682 - accuracy: 0.9490 - val_loss: 0.0846 - val_accuracy: 0.9745 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9508\n",
            "Epoch 10: val_accuracy improved from 0.97450 to 0.97767, saving model to best_model.h5\n",
            "375/375 [==============================] - 18s 47ms/step - loss: 0.1616 - accuracy: 0.9509 - val_loss: 0.0731 - val_accuracy: 0.9777 - lr: 0.0250\n",
            "Epoch 11/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9540"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2fa4a9de3993>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m## Entrenamos con el generador de datos en lugar de con el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),#el .flow sirve para analizar imagenes generadas, es un flujo de imagenes, la cpu se encarga de las modificaciones\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                         \u001b[0m_use_cached_eval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                     )\n\u001b[0;32m-> 1845\u001b[0;31m                     val_logs = {\n\u001b[0m\u001b[1;32m   1846\u001b[0m                         \u001b[0;34m\"val_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                     }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                         \u001b[0m_use_cached_eval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                     )\n\u001b[0;32m-> 1845\u001b[0;31m                     val_logs = {\n\u001b[0m\u001b[1;32m   1846\u001b[0m                         \u001b[0;34m\"val_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                     }\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Implementamos en el ejemplo base el aumentado de datos\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization,Reshape\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30, #de -30 a 30\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,# de -0.8 a 1.2\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest') #cuando lo haces pequeño lo rellenas con lo mas cercano\n",
        "\n",
        "## Importante: ImageDataGenerator espera una imagen con 3 canales, necesitamos hacer reshape\n",
        "x_train = x_train.reshape(48000, 28, 28, 1) #con un canal, sino no funciona, modo de gris\n",
        "x_val = x_val.reshape(12000, 28, 28, 1)\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "\n",
        "## Ajustamos el generador de datos\n",
        "datagen.fit(x_train) #lo calculas si a los datos de entrada le metes alguna transformacion estadistica\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input((28,28,1)))#despues de esto se aplica de forma automatica el image data generator\n",
        "model.add(Reshape((784,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "## Entrenamos con el generador de datos en lugar de con el dataset\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),#el .flow sirve para analizar imagenes generadas, es un flujo de imagenes, la cpu se encarga de las modificaciones\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YKGHuxb3C_T"
      },
      "source": [
        "## Ejercicio:\n",
        "\n",
        "Probar todas las técnicas presentadas para obtener un acierto en **test > 99%**.\n",
        "\n",
        "Se aconseja no malgastar datos de entrenamiento y por lo tanto emplear todo el training set para el entrenamiento. No emplear conjunto de validación y emplear el test set al final para calcular el acierto.\n",
        "\n",
        "A modo de \"trampa\" podríamos ejecutar el fit con los datos de test en validation_data para así monitorizar si llegamos a ese 99%\n",
        "\n",
        "validation_data=(x_test, y_test)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}